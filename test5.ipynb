{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "86a87cdb-a321-48e9-bca2-eb6844e62596",
   "metadata": {
    "id": "86a87cdb-a321-48e9-bca2-eb6844e62596"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.decomposition import PCA\n",
    "import joblib\n",
    "#pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b3cc02c-6727-4ead-9471-29e668102228",
   "metadata": {
    "id": "6b3cc02c-6727-4ead-9471-29e668102228"
   },
   "outputs": [],
   "source": [
    "gm = pd.read_parquet('gmgm.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "91c5ea77-9877-4c5a-9dc6-5c6c8127c44a",
   "metadata": {
    "id": "91c5ea77-9877-4c5a-9dc6-5c6c8127c44a",
    "outputId": "27e20558-c4cf-4653-cbcc-9f4dd9661378"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17143447, 63)\n",
      "protocol_0           float32\n",
      "protocol_6           float32\n",
      "protocol_17          float32\n",
      "flow_duration        float32\n",
      "flow_byts_s          float32\n",
      "flow_pkts_s          float32\n",
      "fwd_pkts_s           float32\n",
      "bwd_pkts_s           float32\n",
      "tot_fwd_pkts         float32\n",
      "tot_bwd_pkts         float32\n",
      "totlen_fwd_pkts      float32\n",
      "totlen_bwd_pkts      float32\n",
      "fwd_pkt_len_max      float32\n",
      "fwd_pkt_len_min      float32\n",
      "fwd_pkt_len_mean     float32\n",
      "fwd_pkt_len_std      float32\n",
      "bwd_pkt_len_max      float32\n",
      "bwd_pkt_len_min      float32\n",
      "bwd_pkt_len_mean     float32\n",
      "bwd_pkt_len_std      float32\n",
      "pkt_len_max          float32\n",
      "pkt_len_min          float32\n",
      "pkt_len_mean         float32\n",
      "pkt_len_std          float32\n",
      "pkt_len_var          float32\n",
      "fwd_seg_size_min     float32\n",
      "fwd_act_data_pkts    float32\n",
      "flow_iat_mean        float32\n",
      "flow_iat_max         float32\n",
      "flow_iat_min         float32\n",
      "flow_iat_std         float32\n",
      "fwd_iat_tot          float32\n",
      "fwd_iat_max          float32\n",
      "fwd_iat_min          float32\n",
      "fwd_iat_mean         float32\n",
      "fwd_iat_std          float32\n",
      "bwd_iat_tot          float32\n",
      "bwd_iat_max          float32\n",
      "bwd_iat_min          float32\n",
      "bwd_iat_mean         float32\n",
      "bwd_iat_std          float32\n",
      "fwd_psh_flags        float32\n",
      "bwd_psh_flags        float32\n",
      "fin_flag_cnt         float32\n",
      "syn_flag_cnt         float32\n",
      "rst_flag_cnt         float32\n",
      "psh_flag_cnt         float32\n",
      "ack_flag_cnt         float32\n",
      "urg_flag_cnt         float32\n",
      "ece_flag_cnt         float32\n",
      "cwr_flag_count       float32\n",
      "down_up_ratio        float32\n",
      "init_fwd_win_byts    float32\n",
      "init_bwd_win_byts    float32\n",
      "active_max           float32\n",
      "active_min           float32\n",
      "active_mean          float32\n",
      "active_std           float32\n",
      "idle_max             float32\n",
      "idle_min             float32\n",
      "idle_mean            float32\n",
      "idle_std             float32\n",
      "Label                  int32\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "pd.set_option('display.max_rows', None)    # Display all rows (in case you use .head())\n",
    "pd.set_option('display.max_columns', None) # Display all columns\n",
    "pd.set_option('display.width', 1000)\n",
    "print(gm.shape)\n",
    "print(gm.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ae8e0318-576b-460c-be27-b9c76c855200",
   "metadata": {
    "id": "ae8e0318-576b-460c-be27-b9c76c855200",
    "outputId": "d1083071-05b2-4aac-b50d-b9aede2d48f9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Separating Label...\n",
      "Separation Done. RAM cleaned.\n"
     ]
    }
   ],
   "source": [
    "import gc # Garbage Collector interface\n",
    "\n",
    "print(\"Separating Label...\")\n",
    "\n",
    "# 1. Rip the 'Label' column out of 'gm' and put it into 'y'.\n",
    "# This modifies 'gm' instantly. 'gm' no longer has the label.\n",
    "y = gm.pop('Label') \n",
    "\n",
    "# 2. Now 'gm' IS your 'X' (features only). Just rename the variable.\n",
    "X = gm \n",
    "\n",
    "# 3. Delete the reference to the name 'gm'\n",
    "del gm \n",
    "\n",
    "# 4. Force Python to release memory immediately\n",
    "gc.collect()\n",
    "\n",
    "print(\"Separation Done. RAM cleaned.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9cbcbe04-3cbe-4315-a08d-7131571f4b36",
   "metadata": {
    "id": "9cbcbe04-3cbe-4315-a08d-7131571f4b36"
   },
   "outputs": [],
   "source": [
    "#split data 80:20 for train and temp\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X, y,\n",
    "    test_size = 0.20,\n",
    "    stratify = y,\n",
    "    random_state = 42,\n",
    "    shuffle = True # shuffle code\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "be615475-dc33-4edb-9a51-e6820f1839e9",
   "metadata": {
    "id": "be615475-dc33-4edb-9a51-e6820f1839e9"
   },
   "outputs": [],
   "source": [
    "#split temp 1:1 for validation and test\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp,\n",
    "    test_size = 0.50,\n",
    "    stratify = y_temp,\n",
    "    random_state = 42,\n",
    "    shuffle = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "07878a82-a4cc-42c1-8edc-32e537ccc6e8",
   "metadata": {
    "id": "be615475-dc33-4edb-9a51-e6820f1839e9"
   },
   "outputs": [],
   "source": [
    "# #log transform (don't run twice)\n",
    "# X_train= np.log1p(X_train)\n",
    "# x_val = np.log1p(X_val)\n",
    "# X_test = np.log1p(X_test)\n",
    "# print('log transform done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5422235b-3c56-4430-86b5-36f12d61183f",
   "metadata": {
    "id": "5422235b-3c56-4430-86b5-36f12d61183f",
    "outputId": "6dcc5b43-276f-43c5-bd82-63d7ed777eac"
   },
   "outputs": [],
   "source": [
    "# #standardscaler (Standardization (Z-Score): ) and not normalization to preserve the shape better for anomaly detection\n",
    "# scaler = StandardScaler()\n",
    "# X_train = scaler.fit_transform(X_train)\n",
    "# X_val = scaler.transform(X_val)\n",
    "# X_test = scaler.transform(X_test)\n",
    "# print(\"standardscaler done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2ebcc900-4c0a-4242-9eaa-1baa5ee44172",
   "metadata": {
    "id": "2ebcc900-4c0a-4242-9eaa-1baa5ee44172",
    "outputId": "2481d118-b296-4c5f-e30c-8bee3bba3d57"
   },
   "outputs": [],
   "source": [
    "# #pca\n",
    "# pca = PCA(n_components = 0.95)\n",
    "# X_train_pca = pca.fit_transform(X_train)\n",
    "# X_val_pca = pca.transform(X_val)\n",
    "# X_test_pca = pca.transform(X_test)\n",
    "# print(\"pca done\")\n",
    "# joblib.dump(pca, 'pca_brain.joblib')\n",
    "# print(\"Step 1 Done: PCA saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c23cdea6-0d62-4862-99ba-e15954ba6d47",
   "metadata": {
    "id": "2iB1UCmLiHj-"
   },
   "outputs": [],
   "source": [
    "Classifier_accuracy = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "de0f47e3-543a-4b28-a457-d1728a5f556b",
   "metadata": {
    "id": "de0f47e3-543a-4b28-a457-d1728a5f556b",
    "outputId": "108e3628-a6f4-4893-b1a5-6cb8755d1c2e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Training Logistic Regression ---\n",
      "Training Time: 5.16 minutes\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.54      0.61    438449\n",
      "           1       0.85      0.92      0.88   1275896\n",
      "\n",
      "    accuracy                           0.82   1714345\n",
      "   macro avg       0.77      0.73      0.75   1714345\n",
      "weighted avg       0.81      0.82      0.81   1714345\n",
      "\n",
      "--- Training Decision Tree ---\n",
      "Training Time: 8.94 minutes\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00    438449\n",
      "           1       1.00      1.00      1.00   1275896\n",
      "\n",
      "    accuracy                           1.00   1714345\n",
      "   macro avg       1.00      1.00      1.00   1714345\n",
      "weighted avg       1.00      1.00      1.00   1714345\n",
      "\n",
      "--- Training Random Forest ---\n",
      "Training Time: 15.24 minutes\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00    438449\n",
      "           1       1.00      1.00      1.00   1275896\n",
      "\n",
      "    accuracy                           1.00   1714345\n",
      "   macro avg       1.00      1.00      1.00   1714345\n",
      "weighted avg       1.00      1.00      1.00   1714345\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#TRAINING DTREE, RF, LOGIRES, CLAS\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "import time\n",
    "\n",
    "# Dictionary of models\n",
    "# n_jobs=-1 means \"Use all CPU cores\"\n",
    "cores = 4\n",
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(class_weight='balanced', max_iter=1000, n_jobs=cores),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(class_weight='balanced', max_depth=20),\n",
    "    \"Random Forest\": RandomForestClassifier(class_weight='balanced', n_estimators=100, max_depth=20, n_jobs=cores)\n",
    "}\n",
    "\n",
    "results = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"--- Training {name} ---\")\n",
    "    start = time.time()\n",
    "\n",
    "    # Train on Full PCA Data\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Predict\n",
    "    y_pred = model.predict(X_val)\n",
    "\n",
    "    end = time.time()\n",
    "    print(f\"Training Time: {(end - start)/60:.2f} minutes\")\n",
    "\n",
    "    # Print Report\n",
    "    print(classification_report(y_val, y_pred))\n",
    "\n",
    "    # Save for comparison\n",
    "    results[name] = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "TsNxtJYLiXap",
   "metadata": {
    "id": "TsNxtJYLiXap"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: Logistic_Regression.joblib\n",
      "Saved: Decision_Tree.joblib\n",
      "Saved: Random_Forest.joblib\n",
      "Step 2 Done: Main models saved!\n"
     ]
    }
   ],
   "source": [
    "############# DI UNCOMMENT NANTI BANG\n",
    "import joblib\n",
    "\n",
    "# This saves the 3 models created in the loop above\n",
    "# We loop through the 'results' dictionary you made\n",
    "if 'results' in locals():\n",
    "    for name, model in results.items():\n",
    "        # Replaces spaces with underscores (e.g. \"Random Forest\" -> \"Random_Forest.joblib\")\n",
    "        filename = name.replace(\" \", \"_\") + \".joblib\"\n",
    "        joblib.dump(model, filename)\n",
    "        print(f\"Saved: {filename}\")\n",
    "else:\n",
    "    print(\"Error: Could not find the 'results' dictionary. Did you run the cell above?\")\n",
    "\n",
    "print(\"Step 2 Done: Main models saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "39ac581e-8509-4976-bce0-c36c81793a95",
   "metadata": {
    "id": "TsNxtJYLiXap"
   },
   "outputs": [],
   "source": [
    "#TRAINING KNN AND SVC\n",
    "\n",
    "###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f4967f75-7456-483d-b520-a97558e0849b",
   "metadata": {
    "id": "f4967f75-7456-483d-b520-a97558e0849b",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Training KNN (On Subset) ---\n",
      "Predicting... (This part might be slow for KNN)\n",
      "Time: 0.51 minutes\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.97      0.97    438449\n",
      "           1       0.99      0.99      0.99   1275896\n",
      "\n",
      "    accuracy                           0.99   1714345\n",
      "   macro avg       0.98      0.98      0.98   1714345\n",
      "weighted avg       0.99      0.99      0.99   1714345\n",
      "\n",
      "--- Training SVM (RBF) (On Subset) ---\n",
      "Predicting... (This part might be slow for KNN)\n",
      "Time: 50.47 minutes\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.26      1.00      0.41    438449\n",
      "           1       0.97      0.00      0.00   1275896\n",
      "\n",
      "    accuracy                           0.26   1714345\n",
      "   macro avg       0.61      0.50      0.21   1714345\n",
      "weighted avg       0.79      0.26      0.11   1714345\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# 1. Create a \"Mini\" dataset just for these slow models\n",
    "# Taking 50,000 rows is usually enough for SVM/KNN to converge\n",
    "X_train_small = X_train[:50000]\n",
    "y_train_small = y_train[:50000]\n",
    "\n",
    "slow_models = {\n",
    "    \"KNN\": KNeighborsClassifier(n_neighbors=5, n_jobs=cores),\n",
    "    \"SVM (RBF)\": SVC(class_weight='balanced', kernel='rbf', cache_size=2000)\n",
    "}\n",
    "\n",
    "for name, model in slow_models.items():\n",
    "    print(f\"--- Training {name} (On Subset) ---\")\n",
    "    start = time.time()\n",
    "\n",
    "    # Train on SMALL Data\n",
    "    model.fit(X_train_small, y_train_small)\n",
    "\n",
    "    # Predict on Validation (also subset if validation is huge, otherwise full is ok but slow)\n",
    "    # Let's predict on full validation to see real performance\n",
    "    print(\"Predicting... (This part might be slow for KNN)\")\n",
    "    y_pred = model.predict(X_val)\n",
    "\n",
    "    end = time.time()\n",
    "    print(f\"Time: {(end - start)/60:.2f} minutes\")\n",
    "    print(classification_report(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "N8Jlxv7AiygZ",
   "metadata": {
    "id": "N8Jlxv7AiygZ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved KNN from dictionary.\n",
      "Saved SVM from dictionary.\n"
     ]
    }
   ],
   "source": [
    "if 'KNN' in slow_models:\n",
    "    joblib.dump(slow_models['KNN'], 'knn_model.joblib')\n",
    "    print(\"Saved KNN from dictionary.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8a2b849a-3e95-413b-85ca-2607792a614b",
   "metadata": {
    "id": "N8Jlxv7AiygZ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved KNN from dictionary.\n",
      "Saved SVM from dictionary.\n"
     ]
    }
   ],
   "source": [
    "if 'SVM (RBF)' in slow_models:\n",
    "    joblib.dump(slow_models['SVM (RBF)'], 'svm_model.joblib')\n",
    "    print(\"Saved SVM from dictionary.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a97023-967d-42df-9e6a-825660c29213",
   "metadata": {
    "id": "N8Jlxv7AiygZ"
   },
   "outputs": [],
   "source": [
    "#TRAINING DNN\n",
    "\n",
    "###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "19b0214f-7e4f-4d78-89ac-7edce71844b9",
   "metadata": {
    "id": "19b0214f-7e4f-4d78-89ac-7edce71844b9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DNN Class Weights: {0: np.float64(1.9550097459996072), 1: np.float64(0.6718201549421563)}\n",
      "--- Training DNN ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/naeem/mmkernel/lib/python3.13/site-packages/keras/src/layers/core/input_layer.py:27: UserWarning: Argument `input_shape` is deprecated. Use `shape` instead.\n",
      "  warnings.warn(\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1766045739.863636 2952656 cuda_executor.cc:1309] INTERNAL: CUDA Runtime error: Failed call to cudaGetRuntimeVersion: Error loading CUDA libraries. GPU will not be used.: Error loading CUDA libraries. GPU will not be used.\n",
      "W0000 00:00:1766045739.870530 2952656 gpu_device.cc:2342] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m6697/6697\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2ms/step - accuracy: 0.3417 - loss: 1106128000.0000 - val_accuracy: 0.2558 - val_loss: 0.6944\n",
      "Epoch 2/10\n",
      "\u001b[1m6697/6697\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2ms/step - accuracy: 0.4445 - loss: 1069231.2500 - val_accuracy: 0.2558 - val_loss: 0.6942\n",
      "Epoch 3/10\n",
      "\u001b[1m6697/6697\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2ms/step - accuracy: 0.5037 - loss: 236227.1094 - val_accuracy: 0.7442 - val_loss: 0.6915\n",
      "Epoch 4/10\n",
      "\u001b[1m6697/6697\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2ms/step - accuracy: 0.5037 - loss: 1838013.6250 - val_accuracy: 0.2558 - val_loss: 0.6940\n",
      "Epoch 5/10\n",
      "\u001b[1m6697/6697\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2ms/step - accuracy: 0.5104 - loss: 1.0239 - val_accuracy: 0.7442 - val_loss: 0.6902\n",
      "Epoch 6/10\n",
      "\u001b[1m6697/6697\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2ms/step - accuracy: 0.5016 - loss: 1.3646 - val_accuracy: 0.2558 - val_loss: 0.6938\n",
      "Epoch 7/10\n",
      "\u001b[1m6697/6697\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2ms/step - accuracy: 0.4988 - loss: 0.7011 - val_accuracy: 0.7442 - val_loss: 0.6931\n",
      "Epoch 8/10\n",
      "\u001b[1m6697/6697\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2ms/step - accuracy: 0.4840 - loss: 0.6958 - val_accuracy: 0.2558 - val_loss: 0.6938\n",
      "Epoch 9/10\n",
      "\u001b[1m6697/6697\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2ms/step - accuracy: 0.5028 - loss: 0.7085 - val_accuracy: 0.7442 - val_loss: 0.6925\n",
      "Epoch 10/10\n",
      "\u001b[1m6697/6697\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2ms/step - accuracy: 0.4963 - loss: 0.6932 - val_accuracy: 0.7442 - val_loss: 0.6910\n",
      "DNN Model Saved.\n",
      "Evaluating on Validation Set...\n",
      "\u001b[1m53574/53574\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 220us/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00    438449\n",
      "           1       0.74      1.00      0.85   1275896\n",
      "\n",
      "    accuracy                           0.74   1714345\n",
      "   macro avg       0.37      0.50      0.43   1714345\n",
      "weighted avg       0.55      0.74      0.64   1714345\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/naeem/mmkernel/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/home/naeem/mmkernel/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/home/naeem/mmkernel/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "\n",
    "# 1. Calculate Class Weights\n",
    "# We handle the imbalance by weighing the minority class higher\n",
    "weights = class_weight.compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.unique(y_train),\n",
    "    y=y_train\n",
    ")\n",
    "dnn_weights = {0: weights[0], 1: weights[1]}\n",
    "print(f\"DNN Class Weights: {dnn_weights}\")\n",
    "\n",
    "# 2. Build the Model\n",
    "# We use X_train.shape[1] to automatically get the number of columns (60+)\n",
    "model_dnn = keras.Sequential([\n",
    "    layers.InputLayer(input_shape=(X_train.shape[1],)), \n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dropout(0.2),\n",
    "    layers.Dense(32, activation='relu'),\n",
    "    layers.Dropout(0.2),\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model_dnn.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# 3. Train\n",
    "print(\"--- Training DNN ---\")\n",
    "# batch_size=2048 is good for both CPU and GPU on large data\n",
    "history = model_dnn.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=10,\n",
    "    batch_size=2048, \n",
    "    class_weight=dnn_weights,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# 4. Save\n",
    "model_dnn.save('dnn_model.keras')\n",
    "print(\"DNN Model Saved.\")\n",
    "\n",
    "# 5. Evaluate\n",
    "print(\"Evaluating on Validation Set...\")\n",
    "y_pred_probs = model_dnn.predict(X_val)\n",
    "# Convert probabilities (0.1, 0.9) to binary labels (0, 1)\n",
    "y_pred_dnn = (y_pred_probs > 0.5).astype(\"int32\")\n",
    "\n",
    "print(classification_report(y_val, y_pred_dnn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fSi4XB89jNah",
   "metadata": {
    "id": "fSi4XB89jNah"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 5 Done: Deep Learning model saved!\n"
     ]
    }
   ],
   "source": [
    "model_dnn.save('dnn_model.keras')\n",
    "print(\"Step 5 Done: Deep Learning model saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "250f9de7-5b60-4c22-b527-ae4505b687fb",
   "metadata": {
    "id": "250f9de7-5b60-4c22-b527-ae4505b687fb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Decision Tree Classifier : 25.67\n"
     ]
    }
   ],
   "source": [
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "Classifier_accuracy.append(accuracy*100)\n",
    "print(\"Accuracy of Decision Tree Classifier : %.2f\" % (accuracy*100) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5052cc5-367b-4da0-b95a-cded88525bd6",
   "metadata": {
    "id": "c5052cc5-367b-4da0-b95a-cded88525bd6"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "010cd302-efc3-4b6a-a7f4-d6f4b3e3c491",
   "metadata": {
    "id": "010cd302-efc3-4b6a-a7f4-d6f4b3e3c491"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed68e0f-8d98-4b17-86d6-92eb96ccd7b8",
   "metadata": {
    "id": "aed68e0f-8d98-4b17-86d6-92eb96ccd7b8"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2420872c-e263-46f1-a3ac-ee1cfb892359",
   "metadata": {
    "id": "2420872c-e263-46f1-a3ac-ee1cfb892359"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ddc605-1dcf-477e-adc3-2698d2c97182",
   "metadata": {
    "id": "55ddc605-1dcf-477e-adc3-2698d2c97182"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e3c7e0-3216-40a2-96b2-2e9b54767f2b",
   "metadata": {
    "id": "53e3c7e0-3216-40a2-96b2-2e9b54767f2b"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf67d41-f370-4b88-b8c5-f303803f8854",
   "metadata": {
    "id": "8cf67d41-f370-4b88-b8c5-f303803f8854"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "markernel",
   "language": "python",
   "name": "mmkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
